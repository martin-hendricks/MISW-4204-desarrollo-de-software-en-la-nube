version: '3.8'

# ============================================
# Docker Compose para Worker con AWS SQS y S3
# ============================================
# Este archivo usa AWS SQS como message broker y S3 para almacenamiento
# Ya NO incluye NFS ni Redis
#
# Uso:
#   1. Configurar .env con credenciales AWS (mismo bucket y colas que Backend)
#   2. Ejecutar ./setup-s3.sh
#   3. docker-compose up -d
# ============================================

services:
  # ===== WORKER - Procesamiento de Videos =====
  worker:
    build:
      context: ../..
      dockerfile: worker/Dockerfile
    container_name: anb-worker
    ports:
      - "8001:8001"  # Health API
    env_file:
      - .env
    networks:
      - worker-network
    restart: unless-stopped
    # Límites de recursos para prevenir bloqueo de la máquina
    deploy:
      resources:
        limits:
          cpus: '2.0'      # Máximo 2 CPUs
          memory: 4096M    # Máximo 4GB RAM
        reservations:
          cpus: '0.5'      # Mínimo 0.5 CPU
          memory: 512M     # Mínimo 512MB RAM
    command: >
      sh -c "python main.py &
             celery -A celery_app worker --loglevel=info --concurrency=2 --queues=video_processing,dlq"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    # ===== CLOUDWATCH LOGS - Auto Scaling Compatible =====
    logging:
      driver: awslogs
      options:
        awslogs-region: us-east-1
        awslogs-group: /aws/ec2/anb-worker
        # awslogs-stream usa el container name (único por instancia EC2)
        # Si hay múltiples instancias, cada una tiene su propio contenedor
        awslogs-stream: worker-${HOSTNAME}
        awslogs-create-group: "true"
        # Modo non-blocking para no afectar performance
        mode: non-blocking
        max-buffer-size: 25m

networks:
  worker-network:
    name: anb-worker-network
    driver: bridge
